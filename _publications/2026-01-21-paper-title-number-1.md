---
title: "从零构建大模型 (Sebastian Raschka)"
collection: publications
category: manuscripts
permalink: /publication/2026-01-21-number-1
excerpt: '1 ■ 了解大型语言模型 1. 2 ■ 处理文本数据 17. 3 ■ 编码注意力机制 50. 4 ■ 从头实现一个GPT模型以生成文本 92. 5 ■ 在未标注数据上预训练 128. 6 ■ 细调用于分类 169. 7 ■ 细调以遵循指令 204'
date: 2026-01-21
venue: ''
# slidesurl: 'https://heiyl.github.io/files/slides1.pdf'
paperurl: 'https://heiyl.github.io/files/book/从零构建大模型 (Sebastian Raschka).pdf'
# bibtexurl: 'https://heiyl.github.io/files/bibtex1.bib'
citation: ''
---
我读完了《从零构建大模型》，这5个核心洞察颠覆了我的认知

引言：揭开LLM的神秘面纱

近几年来，以ChatGPT为代表的大型语言模型（LLM）在全球范围内掀起了一场技术革命。从撰写邮件、总结报告到编写代码，它们强大的能力令人惊叹。然而，对于大多数人来说，这些模型的内部工作原理如同一个神秘的“黑箱”，我们知其然，却不知其所以然。

正是为了揭开这个“黑箱”，Sebastian Raschka撰写了《从零构建大模型》这本堪称“屠龙之术”的实践指南。它并非停留在理论层面，而是带领读者一步步用代码构建一个属于自己的小型GPT模型。通读全书后，我提炼出了五个最具颠覆性、也最有影响力的核心洞察，它们彻底刷新了我对LLM的理解，现在分享给你，让我们一同窥见LLM的底层逻辑。

1. 核心任务出奇的简单：只会“预测下一个词”

我们常常惊叹于LLM能够进行翻译、摘要、问答等复杂任务，直觉上会认为它们的训练目标也一定极其复杂。然而，事实恰恰相反。LLM在预训练阶段的核心任务只有一个，那就是“预测下一个词”。

书中提到：“尽管这是一个非常简单的任务，但许多研究人员对此感到惊讶，因为它能够产生如此强大的模型”。模型在训练时，会看到一段文本，然后被要求预测紧接着的下一个词是什么。这个过程一遍又一遍地在海量文本数据上重复。

这种学习方式被称为“自监督学习”，它的巧妙之处在于，模型能从数据本身“即时”创建标签（即下一个词就是标签），因此可以利用互联网上几乎无限的未标注文本进行训练。正是为了在“预测下一个词”这个简单任务上做到极致，模型被迫学习到了语法规则、上下文逻辑、事实知识，乃至世界运作的某些规律。所有那些令人惊艳的复杂能力，都是从这个简单到极致的目标中“涌现”出来的——这正是我们稍后将要探讨的“涌现能力”的根源。

2. 少即是多：强大的GPT模型只用了Transformer架构的一半

谈到LLM，就绕不开奠定其基础的Transformer架构。2017年提出的原始Transformer架构是为机器翻译任务设计的，它包含两个核心部分：一个负责理解输入文本的“编码器（Encoder）”和一个负责生成输出文本的“解码器（Decoder）”。

然而，一个令人惊讶的事实是，后来的架构根据不同任务进行了战略取舍。例如，用于文本分类和分析的BERT模型，选择了原始架构中的“编码器”部分；而像GPT系列这样强大的生成式模型，则反其道而行之，只使用了“解码器”部分。这是一种架构上的“断舍离”。

书中通过图1.8清晰地展示了这种“仅解码器”（Decoder-only）架构如何以**自回归（autoregressive）**的方式，像玩文字接龙一样，将自己刚刚生成的词作为下一步的输入，来预测再下一个词。这种“少即是多”的设计哲学，砍掉了编码器，反而让模型在文本生成任务上变得更加专注、灵活和高效。GPT的成功证明，有时候简化反而能带来更强大的力量，专注于一个核心功能并将其做到极致，便足以开创一个新时代。

* 3. “无中生有”的超能力：模型的“涌现能力”

LLM最神奇的特性之一，便是它的“涌现能力”（Emergent Abilities）。这意味着模型能够执行它并未被明确训练过的任务。

书中的一个经典例子是翻译能力。一个主要在“下一个词预测”任务上训练的GPT模型，人们意外地发现它竟然具备了不错的翻译能力，尽管训练数据里并没有专门的“原文-译文”配对。这是怎么发生的呢？书中给出了这样的解释：

模型能够执行未明确训练的任务的能力称为新兴行为。这种能力不是在训练期间显式教授的，而是模型暴露于大量多语言数据在不同上下文中时自然产生的结果。

简单来说，模型在阅读了海量的多语言文本后，自己“悟”出了不同语言之间的对应关系和转换模式。这一点至关重要，因为它揭示了LLM的巨大潜力：我们不需要为每一个任务都训练一个专门的模型，一个经过充分预训练的通用大模型，就有可能解决多种不同的问题，这正是通往通用人工智能的重要一步。

4. 语言模型没有“生词”：聪明的字节对编码（BPE）

我们可能会遇到一个问题：如果模型在处理文本时，遇到了一个训练数据里从未见过的词（即词汇表外的词），该怎么办？传统的做法是将其标记为一个特殊的<|unk|>（未知）符号，但这无疑会丢失信息。

而GPT模型采用了一种更聪明的解决方案——字节对编码（Byte-Pair Encoding, BPE）分词器。BPE的核心思想极其巧妙：它不把“词”作为基本单位，而是将所有未知的、复杂的词汇分解为更小的、已知的子词单元，甚至是单个字符。它的工作原理是通过迭代地将文本中频繁出现的字符合并成子词，再将频繁出现的子词合并成更长的词，从而构建起一个高效的词汇表。

书中通过图2.11生动地展示了BPE如何将一个虚构的未知词 "Akwirw ier" 分解为 "Ak"、"w"、"ir"、"w"、" "、"ier" 等多个它认识的子词。这种方法的绝妙之处在于，从根本上消除了“未知词”或“生词”的问题。无论输入多么罕见、多么新颖的单词，甚至是无意义的乱码，BPE都能将其拆解成自己词汇表里的基本组件进行处理。这极大地增强了模型的鲁棒性，确保它能从容应对任何形式的文本输入。

5. 你也可以从零构建一个LLM：揭开黑箱的终极路径

在本书的前言中，作者Sebastian Raschka提出了一个极具启发性的观点，这也是全书的灵魂所在：

我坚信，理解LLMs的最佳方法是从头编写一个——并且你会发现这也可以很有趣！

许多人觉得LLM遥不可及，不仅因为训练成本高昂，更因为其内部机制的复杂性令人望而生畏。而这本书最大的价值，就是提供了一条完整的、动手实践的路径。它从最基础的数据处理、分词、注意力机制编码，到完整的GPT模型架构实现，再到预训练和微调，将整个过程抽丝剥茧，清晰地展现在读者面前。

这个洞察的启发性在于，它赋予了普通技术爱好者和学习者真正掌握这项前沿技术的能力。它告诉我们，理解复杂系统的最佳方式不是旁观和猜测，而是亲手去建造它。通过实践获得的深刻理解，是任何理论学习都无法替代的。LLM的“黑箱”并非无法打开，而这本书就是递到我们手中的那把钥匙。

总结：从原理到实践，LLM并非魔法

通过《从零构建大模型》这本书，我们得以窥见LLM的底层逻辑。从“预测下一个词”这个简单的核心任务，到“仅解码器”的精简架构，再到BPE分词器的巧妙设计，我们发现，LLM并非无法理解的魔法，而是一个建立在一系列巧妙且可理解的原则之上的复杂工程系统。无论是“预测下一个词”的单一目标，还是BPE分词器“化整为零”的巧妙规则，我们都发现，LLM的强大能力源于简单的规则在海量数据和巨大规模下的极致放大。

在了解了这些底层原理之后，你是否也对构建自己的AI应用充满了期待？或者，你认为我们距离真正的通用人工智能还有多远呢？
